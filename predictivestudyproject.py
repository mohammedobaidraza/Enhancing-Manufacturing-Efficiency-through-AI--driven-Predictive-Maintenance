# -*- coding: utf-8 -*-
"""predictivestudyproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10X11J2QaNqCo4HYLefVuhfdrhQzzt4nW
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Data Pre-Processing:"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
df = pd.read_csv('ai4i2020.csv')

import pandas as pd

# Loading the dataset:
df = pd.read_csv('ai4i2020.csv')

# Displaying the first few rows of the dataframe:
print(df.head())

# Checking for missing values:
missing_values = df.isnull().sum()
print(missing_values)

from sklearn.preprocessing import StandardScaler

# Assuming 'df' is our DataFrame and we want to normalize all numeric columns:
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
scaler = StandardScaler()

df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

"""## Feature Engineering & Time-Series Analysis:"""

# Creating new features based on interactions and operational parameters:
df['temp_difference'] = df['Process temperature [K]'] - df['Air temperature [K]']
df['speed_torque_interaction'] = df['Rotational speed [rpm]'] * df['Torque [Nm]']
df['normalized_wear'] = df['Tool wear [min]'] / df['Rotational speed [rpm]']

# Defining the window size
window_size = 5  # The window size can be tuned according to your specific requirements

# Calculating rolling window features for 'Air temperature [K]'
df['air_temp_rolling_mean'] = df['Air temperature [K]'].rolling(window=window_size).mean()
df['air_temp_rolling_std'] = df['Air temperature [K]'].rolling(window=window_size).std()

# Calculating rolling window features for 'Process temperature [K]'
df['process_temp_rolling_mean'] = df['Process temperature [K]'].rolling(window=window_size).mean()
df['process_temp_rolling_std'] = df['Process temperature [K]'].rolling(window=window_size).std()

# Calculating rolling window features for 'Rotational speed [rpm]'
df['rotational_speed_rolling_mean'] = df['Rotational speed [rpm]'].rolling(window=window_size).mean()
df['rotational_speed_rolling_std'] = df['Rotational speed [rpm]'].rolling(window=window_size).std()

# Calculating rolling window features for 'Torque [Nm]'
df['torque_rolling_mean'] = df['Torque [Nm]'].rolling(window=window_size).mean()
df['torque_rolling_std'] = df['Torque [Nm]'].rolling(window=window_size).std()

# Dropping the initial rows where rolling features are NaN due to the window size
df = df.dropna()

# Selecting only the new rolling feature columns for display
rolling_features = df.loc[:, 'air_temp_rolling_mean':'torque_rolling_std']

# Displaying the first few rows of the rolling feature columns
print(rolling_features.head(10))

"""##Explanatory Data Analysis:"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set the aesthetic style of the plots
sns.set_style("whitegrid")

# Plotting the distribution of numerical features
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))

# List of numerical columns to plot
num_columns = ['Air temperature [K]', 'Process temperature [K]',
               'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']

for i, col in enumerate(num_columns):
    sns.histplot(df[col], kde=True, ax=axes[i//2, i%2])

# Adjust the layout
plt.tight_layout()

# Count plot for the target variable 'Machine failure'
plt.figure(figsize=(6, 4))
sns.countplot(x='Machine failure', data=df)

plt.show()

# Now let's check the balance of the target variable
target_balance = df['Machine failure'].value_counts(normalize=True)

target_balance

# Boxplots to examine distribution of numerical features with respect to 'Machine failure'
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))
axes = axes.flatten()  # Flatten the array for easy iteration

for i, col in enumerate(num_columns):
    sns.boxplot(x='Machine failure', y=col, data=df, ax=axes[i])

# Remove the last empty subplot
fig.delaxes(axes[-1])

# Adjust the layout
plt.tight_layout()

# Analysis of 'Type' categorical feature against 'Machine failure'
plt.figure(figsize=(6, 4))
sns.countplot(x='Type', hue='Machine failure', data=df)

plt.show()

# Let's also check the correlations to prepare for multicollinearity check
correlation_matrix = df[num_columns].corr()

# Heatmap of the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', square=True)

plt.show()

# Output the correlation matrix
correlation_matrix

"""## Preliminary Model Evaluation:"""

print(df['Machine failure'].dtype)
print(df['Machine failure'].unique())

"""##Checking for Multicollinearity:"""

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Function to calculate VIF for each feature
def calculate_vif(df, features):
    # Add a constant for the intercept
    X = df[features]
    X['intercept'] = 1

    # Calculate VIF for each feature
    vif = pd.DataFrame()
    vif["Feature"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return vif

# Calculate VIF for the numerical columns
vif_data = calculate_vif(df, num_columns)
vif_data = vif_data[vif_data['Feature'] != 'intercept']  # Remove the intercept row

vif_data

print(y.dtype)
print(sorted(y.unique()))

"""## Implementing Cross-Validated Decision Tree:"""

from sklearn.model_selection import cross_val_predict, KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('/content/ai4i2020.csv')  # Update the path to your CSV file

# Assuming df['Machine failure'] is already binary (0's and 1's)
# If it's not, you should investigate and preprocess accordingly.

# Define the feature columns, make sure these are correct for your dataset
num_columns = df.columns.drop(['Machine failure', 'UDI', 'Product ID', 'Type', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'])
X = df[num_columns]  # Features
y = df['Machine failure']  # Target

# Initialize the Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Cross-validation strategy
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Predictions with cross-validation for the Decision Tree
dt_predictions = cross_val_predict(dt_classifier, X, y, cv=kf)

# Function to evaluate the model
def evaluate_model(name, true_values, predictions):
    accuracy = accuracy_score(true_values, predictions)
    precision = precision_score(true_values, predictions)
    recall = recall_score(true_values, predictions)
    f1 = f1_score(true_values, predictions)
    return {
        'model': name,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }

# Evaluate Decision Tree model
dt_metrics = evaluate_model('Decision Tree', y, dt_predictions)

# Print model metrics
print(dt_metrics)

# Print the first 100 predictions
print(dt_predictions[:100])

"""## SMOTE for Decision Tree:"""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, KFold, cross_val_predict
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score

# Define features and target
X = df[num_columns]
y = df['Machine failure']

# Applying SMOTE
smote = SMOTE(random_state=42)
X_sm, y_sm = smote.fit_resample(X, y)

# Initialize Decision Tree Classifier
dt_classifier_smote = DecisionTreeClassifier(random_state=42)

# For cross-validation with the oversampled data
kf = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv = cross_val_predict(dt_classifier_smote, X_sm, y_sm, cv=kf)

# Performance evaluation with cross-validated predictions
print("Cross-Validation Classification Report:")
print(classification_report(y_sm, y_pred_cv))

# To display some of the predictions
print("Some of the cross-validated predictions:")
print(y_pred_cv[:100])  # Adjust as necessary to display the number of predictions you're interested in

"""## XG-Boost:"""

from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline as make_pipeline_imblearn
from sklearn.model_selection import cross_val_predict, KFold
from xgboost import XGBClassifier
from sklearn.metrics import classification_report

# Clean the feature names by removing or replacing invalid characters
df.columns = [col.replace('[', '').replace(']', '').replace('<', '') for col in df.columns]

# Define the feature columns again, if they were changed during the process
num_columns = df.columns.drop(['Machine failure', 'UDI', 'Product ID', 'Type', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'])

# Define features and target
X = df[num_columns]
y = df['Machine failure']

# Prepare a pipeline that automatically applies SMOTE then fits the XGBoost model
pipeline = make_pipeline_imblearn(SMOTE(random_state=42), XGBClassifier(use_label_encoder=False, eval_metric='logloss'))

# Initialize KFold for cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Use cross_val_predict to generate cross-validated predictions
y_pred_xgb_cv = cross_val_predict(pipeline, X, y, cv=kf)

# Generate a classification report
print("XGBoost with Cross-Validation and SMOTE Classification Report:")
print(classification_report(y, y_pred_xgb_cv))

# Print the first 100 predictions
print("First 100 predictions from the XGBoost model:")
print(y_pred_xgb_cv[:100])

"""## Parameter Tuning & Threshold Adjustment for XG-Boost:"""

from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline as make_pipeline_imblearn
from sklearn.model_selection import cross_val_predict, KFold
from xgboost import XGBClassifier
from sklearn.metrics import classification_report
import numpy as np

# Assuming X and y are already defined
# Calculate the scale_pos_weight
class_counts = y.value_counts()
scale_pos_weight = class_counts[0] / class_counts[1]

# Initialize XGBoost with scale_pos_weight parameter
xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=scale_pos_weight)

# Create pipeline with SMOTE and XGBoost
pipeline = make_pipeline_imblearn(SMOTE(random_state=42), xgb_classifier)

# Initialize KFold for cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Generate cross-validated probability predictions
y_pred_proba = cross_val_predict(pipeline, X, y, cv=kf, method='predict_proba')

# Convert probabilities to 0/1 predictions based on a chosen threshold
threshold = 0.5  # This threshold is hypothetical. Adjust it based on your precision-recall trade-off analysis.
y_pred_threshold = (y_pred_proba[:, 1] > threshold).astype(int)

# Print the classification report for predictions with the custom threshold
print("XGBoost with SMOTE, scale_pos_weight adjustment, and Custom Threshold Classification Report:")
print(classification_report(y, y_pred_threshold))

# Print the first 100 predictions with the custom threshold
print("First 100 predictions with the custom threshold:")
print(y_pred_threshold[:100])

"""##LSTM Implementation:"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.model_selection import train_test_split

# Load the dataset (update path as needed)
df = pd.read_csv('/content/ai4i2020.csv')

# Option 1: Exclude non-numeric columns directly
# Assuming 'Machine failure' is your target and it's already binary
numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()

# Ensure 'Machine failure' is included in numeric_columns if it's excluded by the above
numeric_columns.append('Machine failure') if 'Machine failure' not in numeric_columns else numeric_columns

# Filter the DataFrame to include numeric columns only
df_numeric = df[numeric_columns]

# Option 2: Convert categorical columns to numeric format (optional, not included in this snippet)

# Normalize your features
scaler = StandardScaler()
df_numeric_scaled = scaler.fit_transform(df_numeric.drop('Machine failure', axis=1))

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df_numeric_scaled, df['Machine failure'], test_size=0.2, random_state=42)

# Reshape input to be [samples, time steps, features]
X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=2)

# Predictions
y_pred = model.predict(X_test, verbose=0)
y_pred = (y_pred > 0.5).astype(int).flatten()

# Display the first 100 predictions
print(y_pred[:100])

"""##LSTM after class imbalance handling:

"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
from tensorflow.keras.utils import to_categorical

# Load and preprocess your data
# df = pd.read_csv('/path/to/your/dataset.csv')  # Load your dataset

# Assuming 'Machine failure' is your binary target
y = df['Machine failure'].values
X = df.drop(['Machine failure', 'UDI', 'Product ID', 'Type', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'], axis=1).values

# Normalize X
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Reshape input to be [samples, time steps, features]
X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

from sklearn.utils.class_weight import compute_class_weight

# Compute class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(enumerate(class_weights))

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Fit model with class weights
model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=2, class_weight=class_weight_dict)

# Predict classes (not probabilities)
y_pred = (model.predict(X_test) > 0.5).astype("int32").flatten()

# Display the classification report
print(classification_report(y_test, y_pred))

# Display the first 100 predictions
print(y_pred[:100])

"""##LSTM Using SMOTE to Further balance classes:"""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.metrics import classification_report
import pandas as pd
import numpy as np

# Load and preprocess your dataset
df = pd.read_csv('/content/ai4i2020.csv')  # Update this path
# Prepare your features and target variable
X = df.drop(['Machine failure', 'UDI', 'Product ID', 'Type', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'], axis=1)  # Adjust as needed
y = df['Machine failure']

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Apply SMOTE to the training data
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Reshape input for LSTM [samples, time steps, features]
X_train_smote = X_train_smote.reshape((X_train_smote.shape[0], 1, X_train_smote.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Build the LSTM model
model = Sequential()
model.add(LSTM(units=50, activation='relu', input_shape=(X_train_smote.shape[1], X_train_smote.shape[2])))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_smote, y_train_smote, epochs=10, batch_size=32, verbose=2)

# Predict on the test set
y_pred = (model.predict(X_test) > 0.5).astype(int)

# Classification report
print(classification_report(y_test, y_pred))

# Print the first 100 predictions
print(y_pred.flatten()[:100])